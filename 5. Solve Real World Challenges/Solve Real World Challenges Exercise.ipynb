{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c8aac0",
   "metadata": {},
   "source": [
    "# Credit Card Retention Analysis\n",
    "Dataset Description\n",
    "\n",
    "    A manager at the bank is disturbed with more and more customers leaving their credit card services. They would really like to understand what characteristics lend themselves to someone who is going to churn so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction.\n",
    "\n",
    "    This dataset consists of 10,000 customers mentioning their age, salary, marital_status, credit card limit, credit card category, etc. There are nearly 18 features.\n",
    "\n",
    "    16.07% of customers have churned.\n",
    "\n",
    "In order to read in a csv into python, we will be leveraging the Pandas library. Any package we want to use in Python will need an import statement. In addition to pandas which we will import using import pandas as pd, we will also import matplotlib and seaborn (libraries used for visualization) and numpy (a library for array manipulation).\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec341d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "sns.set()\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2208a973",
   "metadata": {},
   "source": [
    "# Reading in Dataset\n",
    "\n",
    "The method we will be using is pd.read_csv which implies we will be reading a comma separated value file. You can see this in the defaults for this method by typing help(pd.read_csv) and see that the separator is set to , with other helpful defaults like header='infer'. You can read through the rest to get familiar with parameters you can pass through that might be specific to what you may need and different from the defaults.\n",
    "\n",
    "If you type pd.read and then press tab you will see other methods available to you out of the box to read in files. Examples: pd.read_excel, pd.read_pickle, pd.read_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('BankChurners_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0875510",
   "metadata": {},
   "source": [
    "The next steps I always do after reading in my file is to:\n",
    "\n",
    "    1. data.shape to see the size of the dataset. The size will help me decide on how to manage working with the dataset if it happens to be large. Here we see this dataset has 10K+ rows of customer data and 23 columns describing the behavior of those customers.\n",
    "\n",
    "    2. data.head() to see the top of the dataset and make any changes like renaming column names. The default will show the top 5 rows, but you can pass through any number you like (10,25, etc)\n",
    "\n",
    "    3. data.columns to see what all the column names\n",
    "\n",
    "Let's do that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e520fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5559a7e",
   "metadata": {},
   "source": [
    "# General Cleaning Techniques\n",
    "\n",
    "The next important step when working with any dataset, is to perform any necessary cleaning steps. This includes (but is not limited to): 1) converting incorrect variable data types, 2) dropping or imputing missing (NULL) values, 3) finding and fixing erroneous values, and 4) handling outliers.\n",
    "Checking for Duplicates\n",
    "\n",
    "A really good high level check to do at the start is to check for duplicates in the dataset. If there is a unique index you can check on like customer or advertiser IDs then that's the best variable to check uniques on by using the nunique() method. Else, you can use the .drop_duplicates() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CLIENTNUM'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09c423",
   "metadata": {},
   "source": [
    "# Subsetting Data\n",
    "\n",
    "Next, we will look at the column names and see if we want to change column names or subset the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27398740",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['CLIENTNUM', 'Attrition_Flag', 'Customer_Age', 'Gender',\n",
    "       'Dependent_count', 'Education_Level', 'Marital_Status',\n",
    "       'Income_Category', 'Card_Category', 'Months_on_book',\n",
    "       'Total_Relationship_Count', 'Months_Inactive_12_mon',\n",
    "       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',\n",
    "       'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n",
    "       'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29f97a",
   "metadata": {},
   "source": [
    "# Datatypes\n",
    "\n",
    "Next, we will take a look at datatypes. To check for datatypes, we will type df.dtypes to see how each variable has been read in. Main things to check here are dates and numbers that have been read in as string values and will need to be converted into their respective types in order to work with those variables as intended. Here we see that we have three different datatypes int64, float64 and object. The object dtype is roughly analogous to str in native Python. You can reference the user guide for pandas dtypes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe0a1f",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "\n",
    "Next we will check for NULL or missing values in the dataset. We can check this with a simple one-liner. By running the below code, we see that Education Level has 1519 instances of missing vlaues, Marital Status has 749 and Income Category has 1112. Since these are categorical variables, we can replace the null values with Unknown. However, if we were dealing with numerical missing values, we would likely impute (or fill in) the missing records with a mean or median.\n",
    "\n",
    "A more in-depth review of how to handle different types of missing values is here.\n",
    "\n",
    "Let's replace these missing records!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888da259",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Education_Level'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d97f8d",
   "metadata": {},
   "source": [
    "To fill in the missing values, we will use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Education_Level'] = data['Education_Level'].fillna('Unknown')\n",
    "data['Marital_Status'] = data['Marital_Status'].fillna('Unknown')\n",
    "data['Income_Category'] = data['Income_Category'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2b640",
   "metadata": {},
   "source": [
    "We can (and should) check that the code we ran did what we expected, and we can verify like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da16e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f788b22",
   "metadata": {},
   "source": [
    "# Data Transformation: Binning\n",
    "\n",
    "Another part of the data cleaning process can include creating any new variables that we may need later. Here, having each customer's age is helpful, but we could also look at them as groups: people in their 20s, 30s, etc. Let's take a look at how we can create these categories.\n",
    "\n",
    "To bin our ages, we will need a couple data points: the min and max ages. Let's find those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e580b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Customer_Age'].min())\n",
    "print(data['Customer_Age'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7f025",
   "metadata": {},
   "source": [
    "To create bins, we can use the df.cut() method. Parameters we will need to pass in are the bin intervals (which are set to be inclusive by default, but we can override using right=False) and labels for those bins. Thus, we will use the following code to define our age brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b71e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/data-preprocessing-with-python-pandas-part-5-binning-c5bd5fd1b950\n",
    "bins = [25, 30, 40, 50, 60, 70, 80]\n",
    "labels = ['20s', '30s', '40s', '50s', '60s', '70s']\n",
    "data['Customer_Age_bins'] = pd.cut(data['Customer_Age'], bins=bins, labels=labels, include_lowest=True, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b990c",
   "metadata": {},
   "source": [
    "Let's verify that the behavior we expected from this method resulted in the right output. Since we overrode the inclusive values, we should see that 30 should show in the 30s bin vs the 20s bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad45902",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Customer_Age']==30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e53e3",
   "metadata": {},
   "source": [
    "# EDA (exploratory vs explanatory)\n",
    "\n",
    "We will now be moving into what I consider to be the funnest part of any analysis. This is where we will get to explore the data in any which way we want in order to find the \"story\" within the data. Here we know we care about attrition and attributes that are leading to attrition in the hopes of advising the company on how to reduce lost customers.\n",
    "\n",
    "In Cole Knaflic's Storytelling with Data, she covers the difference between exploratory and explanatory analysis and that will be something we emphasize here as well. Typical pitfalls I see when building a final deliverable is to include what was meant to be exploratory analysis. Basically I define this as visuals for ME-- and visuals for YOUR AUDIENCE. Visuals for me can be messy, complex, and anything that helps lead me to the next step or to a conclusion. From there, I can develop a visual for YOUR AUDIENCE that simplifies the finding in a way you can understand in 10 seconds or less.\n",
    "\n",
    "# Sanity Checks\n",
    "\n",
    "Let's start by confirming to ourselves the composition of our client data. A part of any good analysis is continued sanity checks, so let's verify that 16% of our dataset are attrited customers. We can do this simply by looking at counts in each bucket and leveraging the .value_counts() method in python. This method will count the number of instances in the dataset that fall into either category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6020a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Attrition_Flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fe318",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Attrition_Flag'].value_counts()['Attrited Customer'] / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(data['Attrition_Flag'].value_counts()['Attrited Customer'] / data.shape[0] * 100 , 2) , '% of our customers have churned , which matches the documentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c435c81",
   "metadata": {},
   "source": [
    "# Summary Statistics\n",
    "\n",
    "To start to understand our dataset a bit more, we will want to produce summary or descriptive statistics that can tell us more about the variables and features within the dataset. Typically, we are looking to understand:\n",
    "\n",
    "1) how many instances are in the dataset (frequency or counts) \n",
    "2) a measure of central tendency (mean, median, mode)\n",
    "3) the spread of the dataset (variance, standard deviation)\n",
    "\n",
    "Depending on what variable or feature you are looking at, a different measure of central tendency may help represent that data better. It's important to understand the differences between them and when to use which so you can make the right choice.\n",
    "\n",
    "The Mean is the average of all values in a dataset, while the Median represents the midpoint of the values (50% above and 50% below. For example, you have a list y consisting of numbers 0-100 in intervals of 10 as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(range(0, 110, 10))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = sum(y)/len(y) # the way to get the count of a list is by using the len() method\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbec750",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bde5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7744eae",
   "metadata": {},
   "source": [
    "If I add one more data point to the set, 900, let's see how things change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.append(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef73ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580322d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d60c6",
   "metadata": {},
   "source": [
    "Notice how the Mean jumped up, highly impacted by that new data point (outlier), while the median came up slightly, but was much less sensitive. In the case where there are outliers, the Median is a much better representation of central tendency than the Mean.\n",
    "\n",
    "In python, we can use the .describe() method to see these metrics for all the numerical variables in the dataset including: quantiles, min, max and std.\n",
    "\n",
    "std helps us understand how spread out the values of that variable are ---> the bigger the std the bigger the spread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9272e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ead62",
   "metadata": {},
   "source": [
    "Here we can see things like:\n",
    "\n",
    "1) The longest customer in this dataset has been around for 56 months or about 4 years and a half. (Max)\n",
    "2) The average number of relationships a customer has is ~4. (Mean and median agree here)\n",
    "3) The average credit limit is $8.6K, but the median credit limit is much lower at $4.5K. (signals some skew in this variable) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c57df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average Total_Relationship_Count is', round(np.mean(data['Total_Relationship_Count']), 2), 'and the median is', np.median(data['Total_Relationship_Count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average Credit_Limit is $', round(np.mean(data['Credit_Limit']), 2), 'and the median is $', np.median(data['Credit_Limit']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7e0cf",
   "metadata": {},
   "source": [
    "# Distributions\n",
    "\n",
    "Let's now take a look at our three measures of central tendency visually to see how they are impacted by skews and outliers. As you can see, the Mode will always represent the highest point or the point of highest density on the distribution plot. The Median will always split the dataset in half (50/50). And the Mean will lean towards the direction of the skew."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff069195",
   "metadata": {},
   "source": [
    "The most common way to evaluate distributions is through Histograms which look a lot like bar charts, but have very important differences. Histograms specifically look to visualize frequency distributions where the height of the bin will tell you approx how many observations lie within that bin/bucket/range.\n",
    "\n",
    "Let's start with Customer Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['Customer_Age']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b4239",
   "metadata": {},
   "source": [
    "matplotlob has many parameters you can use to customized this chart, but the most important one for histograms are bins. Bins are set to 10 by default and are a way to group the values together to allow a high level distribution to render. However, this is a good one to adjust if the bins are too coarse and are not allowing us to fully see the distribution.\n",
    "\n",
    "Based on our diagrams above, Customer_Age looks fairly normally distributed. Nice, this tells us that it's possible to find normally distributed data in the wild, which is good! To read this chart, the yaxis represents frequencies, or in this case Customers and the xaxis represents the age bins. So, approximately ~2400 customers are within 45-50. This is not meant to be an exact measure, but more an indication of the distribution. To get exact numbers, we will want to dig into the data, but this is our first step to guide us where to look.\n",
    "\n",
    "Let's take a quick look at the distribution for Months_on_book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['Months_on_book']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01613d7",
   "metadata": {},
   "source": [
    "It looks pretty normal outside of a very strong peak at ~36 months (roughly 3500 customers). If we had access to the owner to ask more questions about this, we would want to know if this was a data error (default months for those without a record), or if there was a major marketing campaign that brought in a lot of users 36 months ago.\n",
    "\n",
    "Now let's take a look at the Credit_Limit variable that we looked at before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b73cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['Credit_Limit']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['Credit_Limit']);\n",
    "plt.vlines(data['Credit_Limit'].mean(), 0, 5000)\n",
    "plt.vlines(data['Credit_Limit'].median(), 0, 5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Credit_Limit'].median() # can't see the median line here...let's fix!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1197a4",
   "metadata": {},
   "source": [
    "Let's improve this visual a bit by increasing the bin size, changing the color of the lines and adding some annotations. Next, let's add a title and some x and y labels as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dddde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(data['Credit_Limit'], bins=30);\n",
    "\n",
    "plt.vlines(data['Credit_Limit'].mean(), 0, 2500, colors='Black')\n",
    "plt.vlines(data['Credit_Limit'].median(), 0, 2500, colors='Black')\n",
    "plt.text(data['Credit_Limit'].mean()-1000, 2500+50, \"Mean\")\n",
    "plt.text(data['Credit_Limit'].median()-1000, 2500+50, \"Median\")\n",
    "plt.ylim(0, 2800);\n",
    "plt.title(\"Histogram of Customer Credit Limit\");\n",
    "plt.ylabel('Frequency');\n",
    "plt.xlabel('Credit Limit');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7990bb",
   "metadata": {},
   "source": [
    "MUCH better. Now, it's clear that this Credit Limit is a skewed variable, being skewed higher (or to the right) by a few customers with high Credit Limits.\n",
    "\n",
    "Based on its relation to our feet, we know this is right skewed.\n",
    "\n",
    "Let's try Total_Trans_Ct that looked to have a somewhat similar mean and median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239111ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['Total_Trans_Ct']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e7e52",
   "metadata": {},
   "source": [
    "It might not be obvious that this is a bimodal distribution, let's increase the bin size. Default is 10. Bimodal data typically will show two peaks of data-- which usually indicates youâ€™ve got two different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['Total_Trans_Ct'], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d5f77",
   "metadata": {},
   "source": [
    "# Data Transformations: Normalization and Log\n",
    "\n",
    "While we are still on the topic of distributions, it's important to note that there are simple ways to transform a skewed distribution into one that looks more \"normal\" or to normalize between two values. Let's take a look at a few together.\n",
    "\n",
    "We will cover two very common transformations:\n",
    "\n",
    "1) Normalization (also referred to as a min-max scaler): Normalization will convert all data points to values between two values (usually 0 and 1). \n",
    "2) Log transformation: This helps make the data \"less skewed\". When using log with Python, the default base is usually e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af0a596",
   "metadata": {},
   "source": [
    "We will use the out of the box log transformation from the numpy library and write our own function for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(column):\n",
    "    upper = column.max()\n",
    "    lower = column.min()\n",
    "    y = (column - lower)/(upper-lower)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Credit_Limit_Normalized'] = normalize(data['Credit_Limit'])\n",
    "data['Credit_Limit_Log_Transformed'] = np.log(data['Credit_Limit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec029c4b",
   "metadata": {},
   "source": [
    "To see exactly how the transformations impact the variables we are looking at, we will use seaborn subplots to plot them side by side using subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15,10))\n",
    "fig.suptitle('Before and After Transformation')\n",
    "\n",
    "#create boxplot in each subplot\n",
    "sns.histplot(data, x=\"Credit_Limit\", ax=axes[0,0])\n",
    "sns.histplot(data, x=\"Credit_Limit_Normalized\", ax=axes[0,1])\n",
    "sns.histplot(data, x=\"Credit_Limit\", ax=axes[1,0])\n",
    "sns.histplot(data, x=\"Credit_Limit_Log_Transformed\", ax=axes[1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882adc3",
   "metadata": {},
   "source": [
    "As you can see above, when we Normalized the values, they maintained their shape but now are bound between 0 and 1. This method is helpful when comparing across many variables of different magnitudes (ex. Number of Relationships which ranges between 1-6 vs Credit Limit which ranges between 0 and 35K).\n",
    "\n",
    "However, it's clear that the Log-Transformation made an impact on the distribution!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086cfbd",
   "metadata": {},
   "source": [
    "# Other Distributions Plots\n",
    "Box and Wisker Plot\n",
    "\n",
    "Other ways to look at distributions include Box and Whisker plots, Pyramid Chart, Candlestick charts, etc. We will review Box and Whisker and Pyramid Charts together. Let's look at Total Transaction Count by Gender on a Box and Whisker Plot.\n",
    "\n",
    "The box and whisker plot allows the analyst to quickly find and identify the median, quartiles and any outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ba1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data[\"Gender\"], y=data['Total_Trans_Ct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb403f",
   "metadata": {},
   "source": [
    "This plot quickly shows us that the median transaction account is slightly higher for females than males and outliers seem to be present among both groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29927658",
   "metadata": {},
   "source": [
    "# Pyramid Chart\n",
    "\n",
    "What if the question we wanted to know is if there was a difference in the distribution of customers by age and gender? A Pyramid chart could get us there!\n",
    "\n",
    "For this, we will need to prepare our data. Remember the Customer_Age_bins we made before? We will use them now.\n",
    "\n",
    "To get the data in the format we need, we will need to aggregate the data up to the Age and Gender level. I will do that now with this line of code and we will go over aggregations and groupbys later. For now, let's take a look at the output together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be087f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid = data.groupby(['Gender', 'Customer_Age_bins'])['CLIENTNUM'].nunique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72055e",
   "metadata": {},
   "source": [
    "With a few code line swaps to read in our dataset, we can create the following. This is a small reminder to leverage what exists on the internet. A lot of work will be done for you, and the job will be to know how to find it and translate it for your use-case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_bins = np.array(-1* pyramid[pyramid['Gender']=='F']['CLIENTNUM'])\n",
    "men_bins = np.array(pyramid[pyramid['Gender']=='M']['CLIENTNUM'])\n",
    "\n",
    "y = list(range(20, 100, 10))\n",
    "\n",
    "layout = go.Layout(yaxis=go.layout.YAxis(title='Age'),\n",
    "                   xaxis=go.layout.XAxis(\n",
    "                       range=[-3000, 3000],\n",
    "                       tickvals=[-2500,-2000,-1500,-1000,-500, 0, 500,1000,1500,2000,2500],\n",
    "                       ticktext=[2500,2000,1500,1000,500, 0, 500,1000,1500,2000,2500],\n",
    "                       title='Customers'),\n",
    "                   barmode='overlay',\n",
    "                   bargap=0.1)\n",
    "\n",
    "p_data = [go.Bar(y=y,\n",
    "               x=men_bins,\n",
    "               orientation='h',\n",
    "               name='Men',\n",
    "#                text=men_bins.astype('int'),\n",
    "               hoverinfo='x',\n",
    "               marker=dict(color='powderblue')\n",
    "               ),\n",
    "        go.Bar(y=y,\n",
    "               x=women_bins,\n",
    "               orientation='h',\n",
    "               name='Women',\n",
    "               text=-1 * women_bins.astype('int'),\n",
    "               hoverinfo='text',\n",
    "               marker=dict(color='seagreen')\n",
    "               )]\n",
    "\n",
    "iplot(dict(data=p_data, layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b547a",
   "metadata": {},
   "source": [
    "Here we see there is no real difference in the distribution of Customers by Age and Gender. The majority of customers are in their 40s--a similar finding to the one earlier when we were looking at histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaadd85",
   "metadata": {},
   "source": [
    "# Comparing Categories\n",
    "\n",
    "Comparing histograms to bar charts, let's take a look at the same dimensions we used above (Age and Gender) and use a barplot to visualize these groups. The main difference here will be that we are not looking at frequencies anymore, we are specifying and plotting a measure of our choice. Here, we will look at Credit_Limit by Age and Gender. Let's plot!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b519417",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Customer_Age_bins', y='Credit_Limit', data=data, estimator=np.mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf42f4",
   "metadata": {},
   "source": [
    "Notice how I've passed through an estimator. When the data is given at the Customer level (not aggregated), then it will estimate the values for that group (the default is the mean, but we can switch that out). You'll notice error bars and this is to show that the bar height ends where the mean for that group is, but there are Customers with more or less, as dictated by the length of the error bar.\n",
    "\n",
    "Let's see what happens when we add in Gender, which we can do by specifying the Hue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Customer_Age_bins', y='Credit_Limit', hue='Gender', data=data, estimator=np.mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a18250",
   "metadata": {},
   "source": [
    "Notice how Females are getting credit limits much lower than Males! Interesting insight. Just to sanity check ourselves, let's try the median instead of the mean here. Let's swap that using the estimator parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Customer_Age_bins', y='Credit_Limit', hue='Gender', data=data, estimator=np.median);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefbf73",
   "metadata": {},
   "source": [
    "Looks like it's true that men on average are getting accepted for much more than women are for our company. If we wanted a bar chart without the error bars, we would need to do a quick aggregation. Let's run the next line of code together and go over it in more detail in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b6702",
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot = data.groupby(['Customer_Age_bins', 'Gender'])['Credit_Limit'].mean().reset_index()\n",
    "barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Customer_Age_bins', y='Credit_Limit', hue='Gender', data=barplot);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310fd5e",
   "metadata": {},
   "source": [
    "We can turn the visual so that the bars go horizontally with just one parameter change. Let's try! Here we swap the default orient to \"h\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6206717",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Credit_Limit', y='Customer_Age_bins', hue='Gender', data=barplot, orient='h');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120047ba",
   "metadata": {},
   "source": [
    "There are times where a bar chart may have too many categories that are too close to tell the difference between the heights. A small adaptation from the Bar Chart is the Lollipop Chart, let's make it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada82a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lollipop = data.groupby(['Customer_Age_bins'])['Credit_Limit'].mean().reset_index().sort_values('Credit_Limit')\n",
    "lollipop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55084eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(lollipop['Customer_Age_bins'], lollipop['Credit_Limit'], linefmt='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596f1cb",
   "metadata": {},
   "source": [
    "Most common, is to see this chart with the bars flowing horizontally and sorted from highest to lowest so we can visually pick up how the categories compare with one another. Otherwise they can be more confusing to read so use your best judgement on when to use them. We can do that here with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b70fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jonathansoma.com/lede/foundations-2018/pandas/creating-lollipop-charts-with-pandas-and-matplotlib/\n",
    "fig, ax = plt.subplots()\n",
    "ax.hlines(lollipop['Customer_Age_bins'], xmin=0, xmax=lollipop['Credit_Limit'])\n",
    "ax.plot(lollipop['Credit_Limit'], lollipop['Customer_Age_bins'], \"o\", color='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e626a4",
   "metadata": {},
   "source": [
    "# Data Visualization: Data Tables\n",
    "\n",
    "Aggregations and Groupby: If you are familiar with Pivot Tables in excel, than the way that .groupby() works with pandas dataframes should be pretty intuative! What both of these functions will try to accomplish is aggregations and ways of summarizing the data. They are both flexible enough to allow you to pull in different aggregation types (sum, mean, counts, etc). To read more about the difference between pivot tables in python and groupby, see this guide.\n",
    "\n",
    "Coming back to our business problem, we want to understand something about Customers who are leaving. Let's see if we can see anything obvious about their behavior:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby([ 'Attrition_Flag']).agg({\n",
    "               'CLIENTNUM':'nunique',\n",
    "               'Customer_Age': 'mean', \n",
    "               'Dependent_count': 'mean',\n",
    "               'Months_on_book': 'mean',\n",
    "               'Total_Relationship_Count': 'mean',\n",
    "               'Months_Inactive_12_mon': 'mean',\n",
    "               'Contacts_Count_12_mon': 'mean',\n",
    "               'Credit_Limit': 'mean',\n",
    "               'Total_Revolving_Bal': 'mean',\n",
    "               'Avg_Open_To_Buy': 'mean',\n",
    "               'Total_Amt_Chng_Q4_Q1': 'mean',\n",
    "               'Total_Trans_Amt': 'mean',\n",
    "               'Total_Trans_Ct': 'mean',\n",
    "               'Total_Ct_Chng_Q4_Q1': 'mean',\n",
    "               'Avg_Utilization_Ratio': 'mean'\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c693dd6f",
   "metadata": {},
   "source": [
    "To see the groups more clearly, we can use the Transpose function. This will swap Columns and Rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pivot = data.groupby(['Attrition_Flag']).agg({\n",
    "                           'CLIENTNUM':'nunique', # number of unique customers in each group\n",
    "                           'Customer_Age': 'mean', # the rest are looking at the mean per group\n",
    "                           'Dependent_count': 'mean',\n",
    "                           'Months_on_book': 'mean',\n",
    "                           'Total_Relationship_Count': 'mean',\n",
    "                           'Months_Inactive_12_mon': 'mean',\n",
    "                           'Contacts_Count_12_mon': 'mean',\n",
    "                           'Credit_Limit': 'mean',\n",
    "                           'Total_Revolving_Bal': 'mean',\n",
    "                           'Avg_Open_To_Buy': 'mean',\n",
    "                           'Total_Amt_Chng_Q4_Q1': 'mean',\n",
    "                           'Total_Trans_Amt': 'mean',\n",
    "                           'Total_Trans_Ct': 'mean',\n",
    "                           'Total_Ct_Chng_Q4_Q1': 'mean',\n",
    "                           'Avg_Utilization_Ratio': 'mean'\n",
    "                        })\n",
    "\n",
    "data_pivot.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be56d2",
   "metadata": {},
   "source": [
    "# Relationships\n",
    "\n",
    "The next thing we may be interested in is how variables move with (positively correlated) or against (negatively correlated) each other. We can do this quickly for the whole numerical dataset (or a subset) using a pairplot or pairgrid.\n",
    "\n",
    "This is a nice visualization since it will show relationships between each numerical variable and every other one in the dataset as well as histograms along the diagonal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/25039626/how-do-i-find-numeric-columns-in-pandas\n",
    "numeric_data = data._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de872a",
   "metadata": {},
   "source": [
    "Since this is a computationally expensive operation, and we will not gain new information from some of these variables (like CLLIENTNUM) we will simplify even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9431ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = data[['Credit_Limit', 'Total_Revolving_Bal', 'Months_on_book',\n",
    "       'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n",
    "       'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio', 'Attrition_Flag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef81f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(numeric_data, diag_sharey=False, corner=True)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.map_diag(sns.histplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(x='Total_Trans_Amt', y='Total_Trans_Ct', data=data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f766f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(x='Total_Trans_Amt', y='Total_Trans_Ct', data=data, hue='Attrition_Flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9074ed",
   "metadata": {},
   "source": [
    "Looks like the top group doesn't have a single attrited customer! And that number looks to be around $11K.\n",
    "\n",
    "Finding #1: No attrited customer above $11K of spend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923ccb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
